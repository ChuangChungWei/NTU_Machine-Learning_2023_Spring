{"cells":[{"cell_type":"markdown","metadata":{"id":"OYlaRwNu7ojq"},"source":["# **Homework 2: Phoneme Classification**\n"]},{"cell_type":"markdown","metadata":{"id":"A7DRC5V7_8A5"},"source":["Objectives:\n","* Solve a classification problem with deep neural networks (DNNs).\n","* Understand recursive neural networks (RNNs).\n","\n","If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to mlta-2023-spring@googlegroups.com"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Reference\n","- torch ensemble https://ensemble-pytorch.readthedocs.io/en/latest/"]},{"cell_type":"markdown","metadata":{"id":"KVUGfWTo7_Oj"},"source":["# Download Data\n","Download data from google drive, then unzip it.\n","\n","You should have\n","- `libriphone/train_split.txt`: training metadata\n","- `libriphone/train_labels`: training labels\n","- `libriphone/test_split.txt`: testing metadata\n","- `libriphone/feat/train/*.pt`: training feature\n","- `libriphone/feat/test/*.pt`:  testing feature\n","\n","after running the following block.\n","\n","> **Notes: if the google drive link is dead, you can download the data directly from [Kaggle](https://www.kaggle.com/c/ml2023spring-hw2/data) and upload it to the workspace.**\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-03-07T09:23:08.124200Z","iopub.status.busy":"2023-03-07T09:23:08.123784Z"},"id":"OzkiMEcC3Foq","outputId":"a0c97323-4986-4371-dae1-7950765e8552","trusted":true},"outputs":[{"data":{"text/plain":["\"\\n!pip install --upgrade gdown\\n\\n# Main link\\n# !gdown --id '1N1eVIDe9hKM5uiNRGmifBlwSDGiVXPJe' --output libriphone.zip\\n!gdown --id '1qzCRnywKh30mTbWUEjXuNT2isOCAPdO1' --output libriphone.zip\\n\\n!unzip -q libriphone.zip\\n!ls libriphone\\n\""]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","!pip install --upgrade gdown\n","\n","# Main link\n","# !gdown --id '1N1eVIDe9hKM5uiNRGmifBlwSDGiVXPJe' --output libriphone.zip\n","!gdown --id '1qzCRnywKh30mTbWUEjXuNT2isOCAPdO1' --output libriphone.zip\n","\n","!unzip -q libriphone.zip\n","!ls libriphone\n","'''"]},{"cell_type":"markdown","metadata":{"id":"pADUiYODJE1O"},"source":["# Some Utility Functions\n","**Fixes random number generator seeds for reproducibility.**"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"BsZKgBZQJjaE","trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import random\n","\n","def same_seeds(seed):\n","    random.seed(seed) \n","    np.random.seed(seed)  \n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed) \n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"id":"_L_4anls8Drv"},"source":["**Helper functions to pre-process the training data from raw MFCC features of each utterance.**\n","\n","A phoneme may span several frames and is dependent to past and future frames. \\\n","Hence we concatenate neighboring phonemes for training to achieve higher accuracy. The **concat_feat** function concatenates past and future k frames (total 2k+1 = n frames), and we predict the center frame.\n","\n","Feel free to modify the data preprocess functions, but **do not drop any frame** (if you modify the functions, remember to check that the number of frames are the same as mentioned in the slides)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"IJjLT8em-y9G","trusted":true},"outputs":[],"source":["import os\n","import torch\n","from tqdm import tqdm\n","\n","def load_feat(path):\n","    feat = torch.load(path)\n","    return feat\n","\n","def shift(x, n):\n","    if n < 0:\n","        left = x[0].repeat(-n, 1)\n","        right = x[:n]\n","    elif n > 0:\n","        right = x[-1].repeat(n, 1)\n","        left = x[n:]\n","    else:\n","        return x\n","\n","    return torch.cat((left, right), dim=0)\n","\n","def concat_feat(x, concat_n):\n","    assert concat_n % 2 == 1 # n must be odd\n","    if concat_n < 2:\n","        return x\n","    seq_len, feature_dim = x.size(0), x.size(1)\n","    x = x.repeat(1, concat_n) \n","    x = x.view(seq_len, concat_n, feature_dim).permute(1, 0, 2) # concat_n, seq_len, feature_dim\n","    mid = (concat_n // 2)\n","    for r_idx in range(1, mid+1):\n","        x[mid + r_idx, :] = shift(x[mid + r_idx], r_idx)\n","        x[mid - r_idx, :] = shift(x[mid - r_idx], -r_idx)\n","\n","    return x.permute(1, 0, 2).view(seq_len, concat_n * feature_dim)\n","\n","def preprocess_data(split, feat_dir, phone_path, concat_nframes, train_ratio=0.8, random_seed=1213):\n","    class_num = 41 # NOTE: pre-computed, should not need change\n","\n","    if split == 'train' or split == 'val':\n","        mode = 'train'\n","    elif split == 'test':\n","        mode = 'test'\n","    else:\n","        raise ValueError('Invalid \\'split\\' argument for dataset: PhoneDataset!')\n","\n","    label_dict = {}\n","    if mode == 'train':\n","        for line in open(os.path.join(phone_path, f'{mode}_labels.txt')).readlines():\n","            line = line.strip('\\n').split(' ')\n","            label_dict[line[0]] = [int(p) for p in line[1:]]\n","        \n","        # split training and validation data\n","        usage_list = open(os.path.join(phone_path, 'train_split.txt')).readlines()\n","        random.seed(random_seed)\n","        random.shuffle(usage_list)\n","        train_len = int(len(usage_list) * train_ratio)\n","        usage_list = usage_list[:train_len] if split == 'train' else usage_list[train_len:]\n","\n","    elif mode == 'test':\n","        usage_list = open(os.path.join(phone_path, 'test_split.txt')).readlines()\n","\n","    usage_list = [line.strip('\\n') for line in usage_list]\n","    print('[Dataset] - # phone classes: ' + str(class_num) + ', number of utterances for ' + split + ': ' + str(len(usage_list)))\n","\n","    max_len = 3000000\n","    X = torch.empty(max_len, 39 * concat_nframes)\n","    if mode == 'train':\n","        y = torch.empty(max_len, dtype=torch.long)\n","\n","    idx = 0\n","    for i, fname in tqdm(enumerate(usage_list)):\n","        feat = load_feat(os.path.join(feat_dir, mode, f'{fname}.pt'))\n","        cur_len = len(feat)\n","        feat = concat_feat(feat, concat_nframes)\n","        if mode == 'train':\n","          label = torch.LongTensor(label_dict[fname])\n","\n","        X[idx: idx + cur_len, :] = feat\n","        if mode == 'train':\n","          y[idx: idx + cur_len] = label\n","\n","        idx += cur_len\n","\n","    X = X[:idx, :]\n","    if mode == 'train':\n","      y = y[:idx]\n","\n","    print(f'[INFO] {split} set')\n","    print(X.shape)\n","    if mode == 'train':\n","      print(y.shape)\n","      return X, y\n","    else:\n","      return X\n"]},{"cell_type":"markdown","metadata":{"id":"us5XW_x6udZQ"},"source":["# Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Fjf5EcmJtf4e","trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","\n","class LibriDataset(Dataset):\n","    def __init__(self, X, y=None):\n","        self.data = X\n","        if y is not None:\n","            self.label = torch.LongTensor(y)\n","        else:\n","            self.label = None\n","\n","    def __getitem__(self, idx):\n","        if self.label is not None:\n","            return self.data[idx], self.label[idx]\n","        else:\n","            return self.data[idx]\n","\n","    def __len__(self):\n","        return len(self.data)\n"]},{"cell_type":"markdown","metadata":{"id":"IRqKNvNZwe3V"},"source":["# Model\n","Feel free to modify the structure of the model."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## model 1"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class BasicBlock(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(BasicBlock, self).__init__()\n","\n","        # TODO: apply batch normalization and dropout for strong baseline.\n","        # Reference: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html (batch normalization)\n","        #       https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html (dropout)\n","        self.block = nn.Sequential(\n","            nn.Linear(input_dim, output_dim),\n","            nn.BatchNorm1d(output_dim),\n","            nn.Dropout(0.2),\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.block(x)\n","        return x\n","\n","\n","class Classifier(nn.Module):\n","    def __init__(self, input_dim, output_dim=41, hidden_layers=1, hidden_dim=256):\n","        super(Classifier, self).__init__()\n","        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n","            input_size=39*nframes,\n","            hidden_size=256,         # rnn hidden unit\n","            num_layers=4,  # number of rnn layer\n","            batch_first=True, \n","            bidirectional=True,# input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n","        )\n","        self.fc = nn.Sequential(\n","            #nn.BatchNorm1d(120),\n","            BasicBlock(512, hidden_dim),\n","            *[BasicBlock(hidden_dim, hidden_dim) for _ in range(hidden_layers)],\n","            #nn.Dropout(0.1),\n","            #nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, output_dim)\n","        )\n","        self.fc2 = nn.Sequential(\n","            #nn.BatchNorm1d(700),\n","            #BasicBlock(120, hidden_dim),\n","            #*[BasicBlock(hidden_dim, hidden_dim) for _ in range(hidden_layers)],\n","            #nn.Dropout(0.1),\n","            #nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(700 ,output_dim)\n","        )\n","        self.dropout=nn.Dropout(0.1)\n","        \n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(1, 256, 10, 1, 1),  #[1,40,40]# [64, 128, 128]\n","            #nn.BatchNorm2d(1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),      #[1,20,20]\n","            \n","            nn.Conv2d(256, 1, 10, 1, 1),  #[1,40,40]# [64, 128, 128]\n","        )\n","\n","    def forward(self, x):\n","        #x=self.dropout(x)\n","        '''\n","        x=torch.unsqueeze(x,1)\n","        #print(x.size())\n","        x=self.cnn(x)\n","        \n","        #print(x.size())\n","        x=torch.squeeze(x,1)\n","        '''\n","        r_out, h_n = self.rnn(x, None)\n","        \n","        #r_out=torch.squeeze(r_out)\n","        r_out=r_out[:,-1,:].squeeze()\n","        r_out=self.fc(r_out)\n","        return r_out"]},{"cell_type":"markdown","metadata":{"id":"TlIq8JeqvvHC"},"source":["# Hyper-parameters"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"id":"Wc0g-JbikYZO","outputId":"16503867-2b46-47e0-f5ef-df4f87011a9d","trusted":true},"outputs":[],"source":["from datetime import datetime\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","currentDateAndTime = str(datetime.now())\n","#currentDateAndTime = '2023-03-05 22:29:51.527292'\n","try:\n","    os.mkdir(\"model\")\n","except:\n","    pass\n","rnn_dim=41\n","nframes=1\n","concat_nframes=nframes*rnn_dim\n","\n","config = {\n","    'concat_nframes': concat_nframes ,    # Your seed number, you can pick your lucky number. :)\n","    'train_ratio': 0.9,   # Whether to use all features.\n","    'seed': 1213,   # validation_size = train_size * valid_ratio\n","    'batch_size': 512,     # Number of epochs.            \n","    'num_epoch': 8, \n","    'learning_rate': 1e-3,              \n","    'model_path': './model/model_'+currentDateAndTime+'.ckpt',    # If model has not improved for this many consecutive epochs, stop training.     \n","    'input_dim': 39 * concat_nframes , # Your model will be saved herE\n","    'hidden_layers': 3,\n","    'hidden_dim': 256,\n","    'nframes':nframes\n","}\n","\n","'''\n","# data prarameters\n","concat_nframes = 3              # the number of frames to concat with, n must be odd (total 2k+1 = n frames)\n","train_ratio = 0.75               # the ratio of data used for training, the rest will be used for validation\n","\n","# training parameters\n","seed = 1213                        # random seed\n","batch_size = 512                # batch size\n","num_epoch = 10                   # the number of training epoch\n","learning_rate = 1e-4         # learning rate\n","model_path = './model.ckpt'     # the path where the checkpoint will be saved\n","\n","# model parameters\n","input_dim = 39 * concat_nframes # the input dim of the model, you should not change the value\n","hidden_layers = 2               # the number of hidden layers\n","hidden_dim = 64                # the hidden dim\n","'''\n","# data prarameters\n","# TODO: change the value of \"concat_nframes\" for medium baseline\n","concat_nframes = config['concat_nframes']   # the number of frames to concat with, n must be odd (total 2k+1 = n frames)\n","train_ratio = config['train_ratio']   # the ratio of data used for training, the rest will be used for validation\n","\n","# training parameters\n","seed =config['seed']         # random seed\n","batch_size = config['batch_size']        # batch size\n","num_epoch = config['num_epoch']      # the number of training epoch\n","learning_rate = config['learning_rate']      # learning rate\n","model_path = config['model_path']    # the path where the checkpoint will be saved\n","\n","\n","# model parameters\n","# TODO: change the value of \"hidden_layers\" or \"hidden_dim\" for medium baseline\n","input_dim = config['input_dim']   # the input dim of the model, you should not change the value\n","hidden_layers = config['hidden_layers']    # the number of hidden layers\n","hidden_dim = config['hidden_dim']            # the hidden dim\n"]},{"cell_type":"markdown","metadata":{"id":"IIUFRgG5yoDn"},"source":["# Dataloader"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1zI3v5jyrDn","outputId":"2c532cca-4006-48dc-fcda-6089d536ee86","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DEVICE: cuda\n","[Dataset] - # phone classes: 41, number of utterances for train: 3086\n"]},{"name":"stderr","output_type":"stream","text":["3086it [01:02, 49.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[INFO] train set\n","torch.Size([1906228, 1599])\n","torch.Size([1906228])\n","[Dataset] - # phone classes: 41, number of utterances for val: 343\n"]},{"name":"stderr","output_type":"stream","text":["343it [00:07, 44.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[INFO] val set\n","torch.Size([210566, 1599])\n","torch.Size([210566])\n"]}],"source":["from torch.utils.data import DataLoader\n","import gc\n","\n","same_seeds(seed)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'DEVICE: {device}')\n","\n","# preprocess data\n","train_X, train_y = preprocess_data(split='train', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes, train_ratio=train_ratio, random_seed=seed)\n","val_X, val_y = preprocess_data(split='val', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes, train_ratio=train_ratio, random_seed=seed)\n","\n","# get dataset\n","train_X=train_X.reshape(-1,rnn_dim,nframes*39)\n","\n","val_X=val_X.reshape(-1,rnn_dim,nframes*39)\n","\n","train_set = LibriDataset(train_X, train_y)\n","val_set = LibriDataset(val_X, val_y)\n","\n","\n","# remove raw feature to save memory\n","del train_X, train_y, val_X, val_y\n","gc.collect()\n","#dataset = torch.utils.data.ConcatDataset([train_set, val_set])\n","# get dataloader\n","train_loader= DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n","dataset = torch.utils.data.ConcatDataset([train_set, val_set])"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["dataset = torch.utils.data.ConcatDataset([train_set, val_set])"]},{"cell_type":"markdown","metadata":{"id":"pwWH1KIqzxEr"},"source":["# Training"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Trainer3\n","The trainer3 implements the k-fold cross validation. I use the voting ensemble model made by Ensemble PyTorch."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fold 1\n","2023-03-22 13:31:44.833729\n","Estimator: 000 | Epoch: 000 | Batch: 000 | Loss: 3.80497 | Correct: 10/512\n","Estimator: 000 | Epoch: 000 | Batch: 100 | Loss: 2.37848 | Correct: 163/512\n","Estimator: 000 | Epoch: 000 | Batch: 200 | Loss: 2.04859 | Correct: 213/512\n","Estimator: 000 | Epoch: 000 | Batch: 300 | Loss: 1.91038 | Correct: 224/512\n","Estimator: 000 | Epoch: 000 | Batch: 400 | Loss: 1.82815 | Correct: 242/512\n","Estimator: 000 | Epoch: 000 | Batch: 500 | Loss: 1.52701 | Correct: 272/512\n","Estimator: 000 | Epoch: 000 | Batch: 600 | Loss: 1.56225 | Correct: 280/512\n","Estimator: 000 | Epoch: 000 | Batch: 700 | Loss: 1.52413 | Correct: 281/512\n","Estimator: 000 | Epoch: 000 | Batch: 800 | Loss: 1.29621 | Correct: 324/512\n","Estimator: 000 | Epoch: 000 | Batch: 900 | Loss: 1.31743 | Correct: 316/512\n","Estimator: 000 | Epoch: 000 | Batch: 1000 | Loss: 1.13345 | Correct: 345/512\n","Estimator: 000 | Epoch: 000 | Batch: 1100 | Loss: 1.19034 | Correct: 336/512\n","Estimator: 000 | Epoch: 000 | Batch: 1200 | Loss: 1.07743 | Correct: 335/512\n","Estimator: 000 | Epoch: 000 | Batch: 1300 | Loss: 1.05930 | Correct: 351/512\n","Estimator: 000 | Epoch: 000 | Batch: 1400 | Loss: 1.02352 | Correct: 350/512\n","Estimator: 000 | Epoch: 000 | Batch: 1500 | Loss: 1.07090 | Correct: 355/512\n","Estimator: 000 | Epoch: 000 | Batch: 1600 | Loss: 1.00370 | Correct: 354/512\n","Estimator: 000 | Epoch: 000 | Batch: 1700 | Loss: 1.07676 | Correct: 355/512\n","Estimator: 000 | Epoch: 000 | Batch: 1800 | Loss: 0.94304 | Correct: 383/512\n","Estimator: 000 | Epoch: 000 | Batch: 1900 | Loss: 0.95056 | Correct: 375/512\n","Estimator: 000 | Epoch: 000 | Batch: 2000 | Loss: 0.94173 | Correct: 371/512\n","Estimator: 000 | Epoch: 000 | Batch: 2100 | Loss: 0.90602 | Correct: 371/512\n","Estimator: 000 | Epoch: 000 | Batch: 2200 | Loss: 0.96153 | Correct: 379/512\n","Estimator: 000 | Epoch: 000 | Batch: 2300 | Loss: 0.86502 | Correct: 384/512\n","Estimator: 000 | Epoch: 000 | Batch: 2400 | Loss: 0.95343 | Correct: 362/512\n","Estimator: 000 | Epoch: 000 | Batch: 2500 | Loss: 0.76754 | Correct: 411/512\n","Estimator: 000 | Epoch: 000 | Batch: 2600 | Loss: 0.92264 | Correct: 372/512\n","Estimator: 000 | Epoch: 000 | Batch: 2700 | Loss: 0.89206 | Correct: 380/512\n","Estimator: 000 | Epoch: 000 | Batch: 2800 | Loss: 0.84073 | Correct: 389/512\n","Estimator: 000 | Epoch: 000 | Batch: 2900 | Loss: 0.81591 | Correct: 388/512\n","Estimator: 000 | Epoch: 000 | Batch: 3000 | Loss: 0.87738 | Correct: 383/512\n","Estimator: 000 | Epoch: 000 | Batch: 3100 | Loss: 0.85289 | Correct: 393/512\n","Estimator: 000 | Epoch: 000 | Batch: 3200 | Loss: 0.80648 | Correct: 394/512\n","Estimator: 000 | Epoch: 000 | Batch: 3300 | Loss: 0.77906 | Correct: 407/512\n","Fold 2\n","2023-03-22 13:37:06.639444\n","Estimator: 000 | Epoch: 000 | Batch: 000 | Loss: 3.85140 | Correct: 14/512\n","Estimator: 000 | Epoch: 000 | Batch: 100 | Loss: 2.49438 | Correct: 158/512\n","Estimator: 000 | Epoch: 000 | Batch: 200 | Loss: 2.04861 | Correct: 209/512\n","Estimator: 000 | Epoch: 000 | Batch: 300 | Loss: 1.92652 | Correct: 218/512\n","Estimator: 000 | Epoch: 000 | Batch: 400 | Loss: 1.81301 | Correct: 244/512\n","Estimator: 000 | Epoch: 000 | Batch: 500 | Loss: 1.55949 | Correct: 277/512\n","Estimator: 000 | Epoch: 000 | Batch: 600 | Loss: 1.38438 | Correct: 288/512\n","Estimator: 000 | Epoch: 000 | Batch: 700 | Loss: 1.47030 | Correct: 304/512\n","Estimator: 000 | Epoch: 000 | Batch: 800 | Loss: 1.26155 | Correct: 323/512\n","Estimator: 000 | Epoch: 000 | Batch: 900 | Loss: 1.26880 | Correct: 324/512\n","Estimator: 000 | Epoch: 000 | Batch: 1000 | Loss: 1.20889 | Correct: 333/512\n","Estimator: 000 | Epoch: 000 | Batch: 1100 | Loss: 1.26361 | Correct: 320/512\n","Estimator: 000 | Epoch: 000 | Batch: 1200 | Loss: 1.11431 | Correct: 341/512\n","Estimator: 000 | Epoch: 000 | Batch: 1300 | Loss: 1.06472 | Correct: 357/512\n","Estimator: 000 | Epoch: 000 | Batch: 1400 | Loss: 1.05066 | Correct: 353/512\n","Estimator: 000 | Epoch: 000 | Batch: 1500 | Loss: 1.06279 | Correct: 361/512\n","Estimator: 000 | Epoch: 000 | Batch: 1600 | Loss: 1.06459 | Correct: 358/512\n","Estimator: 000 | Epoch: 000 | Batch: 1700 | Loss: 1.01250 | Correct: 355/512\n","Estimator: 000 | Epoch: 000 | Batch: 1800 | Loss: 0.98829 | Correct: 348/512\n","Estimator: 000 | Epoch: 000 | Batch: 1900 | Loss: 1.07940 | Correct: 346/512\n","Estimator: 000 | Epoch: 000 | Batch: 2000 | Loss: 0.99365 | Correct: 372/512\n","Estimator: 000 | Epoch: 000 | Batch: 2100 | Loss: 0.97029 | Correct: 376/512\n","Estimator: 000 | Epoch: 000 | Batch: 2200 | Loss: 0.98177 | Correct: 365/512\n","Estimator: 000 | Epoch: 000 | Batch: 2300 | Loss: 0.84151 | Correct: 392/512\n","Estimator: 000 | Epoch: 000 | Batch: 2400 | Loss: 0.86186 | Correct: 387/512\n","Estimator: 000 | Epoch: 000 | Batch: 2500 | Loss: 0.93269 | Correct: 362/512\n","Estimator: 000 | Epoch: 000 | Batch: 2600 | Loss: 0.93177 | Correct: 369/512\n","Estimator: 000 | Epoch: 000 | Batch: 2700 | Loss: 0.94996 | Correct: 367/512\n","Estimator: 000 | Epoch: 000 | Batch: 2800 | Loss: 0.82400 | Correct: 386/512\n","Estimator: 000 | Epoch: 000 | Batch: 2900 | Loss: 0.74890 | Correct: 396/512\n","Estimator: 000 | Epoch: 000 | Batch: 3000 | Loss: 0.81095 | Correct: 385/512\n","Estimator: 000 | Epoch: 000 | Batch: 3100 | Loss: 0.89113 | Correct: 380/512\n","Estimator: 000 | Epoch: 000 | Batch: 3200 | Loss: 0.66685 | Correct: 416/512\n","Estimator: 000 | Epoch: 000 | Batch: 3300 | Loss: 0.82999 | Correct: 383/512\n","Fold 3\n","2023-03-22 13:42:17.803750\n","Estimator: 000 | Epoch: 000 | Batch: 000 | Loss: 3.86078 | Correct: 12/512\n","Estimator: 000 | Epoch: 000 | Batch: 100 | Loss: 2.41650 | Correct: 172/512\n","Estimator: 000 | Epoch: 000 | Batch: 200 | Loss: 2.09134 | Correct: 216/512\n","Estimator: 000 | Epoch: 000 | Batch: 300 | Loss: 1.84697 | Correct: 241/512\n","Estimator: 000 | Epoch: 000 | Batch: 400 | Loss: 1.62705 | Correct: 256/512\n","Estimator: 000 | Epoch: 000 | Batch: 500 | Loss: 1.63945 | Correct: 263/512\n","Estimator: 000 | Epoch: 000 | Batch: 600 | Loss: 1.58615 | Correct: 277/512\n","Estimator: 000 | Epoch: 000 | Batch: 700 | Loss: 1.44072 | Correct: 290/512\n","Estimator: 000 | Epoch: 000 | Batch: 800 | Loss: 1.35793 | Correct: 306/512\n","Estimator: 000 | Epoch: 000 | Batch: 900 | Loss: 1.34793 | Correct: 307/512\n","Estimator: 000 | Epoch: 000 | Batch: 1000 | Loss: 1.29124 | Correct: 327/512\n","Estimator: 000 | Epoch: 000 | Batch: 1100 | Loss: 1.29919 | Correct: 327/512\n","Estimator: 000 | Epoch: 000 | Batch: 1200 | Loss: 1.26070 | Correct: 322/512\n","Estimator: 000 | Epoch: 000 | Batch: 1300 | Loss: 1.28798 | Correct: 330/512\n","Estimator: 000 | Epoch: 000 | Batch: 1400 | Loss: 1.28658 | Correct: 317/512\n","Estimator: 000 | Epoch: 000 | Batch: 1500 | Loss: 0.99770 | Correct: 364/512\n","Estimator: 000 | Epoch: 000 | Batch: 1600 | Loss: 1.00726 | Correct: 365/512\n","Estimator: 000 | Epoch: 000 | Batch: 1700 | Loss: 0.97147 | Correct: 359/512\n","Estimator: 000 | Epoch: 000 | Batch: 1800 | Loss: 0.95192 | Correct: 365/512\n","Estimator: 000 | Epoch: 000 | Batch: 1900 | Loss: 1.04607 | Correct: 369/512\n","Estimator: 000 | Epoch: 000 | Batch: 2000 | Loss: 1.06626 | Correct: 361/512\n","Estimator: 000 | Epoch: 000 | Batch: 2100 | Loss: 0.89902 | Correct: 377/512\n","Estimator: 000 | Epoch: 000 | Batch: 2200 | Loss: 1.07328 | Correct: 350/512\n","Estimator: 000 | Epoch: 000 | Batch: 2300 | Loss: 1.01402 | Correct: 369/512\n","Estimator: 000 | Epoch: 000 | Batch: 2400 | Loss: 0.96181 | Correct: 372/512\n","Estimator: 000 | Epoch: 000 | Batch: 2500 | Loss: 0.80683 | Correct: 391/512\n","Estimator: 000 | Epoch: 000 | Batch: 2600 | Loss: 0.85599 | Correct: 394/512\n","Estimator: 000 | Epoch: 000 | Batch: 2700 | Loss: 0.83583 | Correct: 383/512\n","Estimator: 000 | Epoch: 000 | Batch: 2800 | Loss: 0.84972 | Correct: 374/512\n","Estimator: 000 | Epoch: 000 | Batch: 2900 | Loss: 0.87829 | Correct: 377/512\n","Estimator: 000 | Epoch: 000 | Batch: 3000 | Loss: 0.88307 | Correct: 385/512\n","Estimator: 000 | Epoch: 000 | Batch: 3100 | Loss: 0.94480 | Correct: 367/512\n","Estimator: 000 | Epoch: 000 | Batch: 3200 | Loss: 0.77536 | Correct: 397/512\n","Estimator: 000 | Epoch: 000 | Batch: 3300 | Loss: 0.73729 | Correct: 393/512\n","Fold 4\n","2023-03-22 13:47:29.726727\n","Estimator: 000 | Epoch: 000 | Batch: 000 | Loss: 3.80545 | Correct: 9/512\n","Estimator: 000 | Epoch: 000 | Batch: 100 | Loss: 2.45137 | Correct: 170/512\n","Estimator: 000 | Epoch: 000 | Batch: 200 | Loss: 2.08044 | Correct: 194/512\n","Estimator: 000 | Epoch: 000 | Batch: 300 | Loss: 1.80109 | Correct: 236/512\n","Estimator: 000 | Epoch: 000 | Batch: 400 | Loss: 1.69502 | Correct: 245/512\n","Estimator: 000 | Epoch: 000 | Batch: 500 | Loss: 1.60230 | Correct: 260/512\n","Estimator: 000 | Epoch: 000 | Batch: 600 | Loss: 1.47578 | Correct: 286/512\n","Estimator: 000 | Epoch: 000 | Batch: 700 | Loss: 1.36927 | Correct: 312/512\n","Estimator: 000 | Epoch: 000 | Batch: 800 | Loss: 1.35991 | Correct: 310/512\n","Estimator: 000 | Epoch: 000 | Batch: 900 | Loss: 1.23527 | Correct: 327/512\n","Estimator: 000 | Epoch: 000 | Batch: 1000 | Loss: 1.27763 | Correct: 310/512\n","Estimator: 000 | Epoch: 000 | Batch: 1100 | Loss: 1.14309 | Correct: 337/512\n","Estimator: 000 | Epoch: 000 | Batch: 1200 | Loss: 1.38232 | Correct: 318/512\n","Estimator: 000 | Epoch: 000 | Batch: 1300 | Loss: 1.02023 | Correct: 361/512\n","Estimator: 000 | Epoch: 000 | Batch: 1400 | Loss: 1.11654 | Correct: 351/512\n","Estimator: 000 | Epoch: 000 | Batch: 1500 | Loss: 1.03539 | Correct: 362/512\n","Estimator: 000 | Epoch: 000 | Batch: 1600 | Loss: 0.96121 | Correct: 376/512\n","Estimator: 000 | Epoch: 000 | Batch: 1700 | Loss: 1.03295 | Correct: 349/512\n","Estimator: 000 | Epoch: 000 | Batch: 1800 | Loss: 0.96931 | Correct: 366/512\n","Estimator: 000 | Epoch: 000 | Batch: 1900 | Loss: 1.07436 | Correct: 350/512\n","Estimator: 000 | Epoch: 000 | Batch: 2000 | Loss: 0.90633 | Correct: 366/512\n","Estimator: 000 | Epoch: 000 | Batch: 2100 | Loss: 0.90317 | Correct: 377/512\n","Estimator: 000 | Epoch: 000 | Batch: 2200 | Loss: 0.98391 | Correct: 368/512\n","Estimator: 000 | Epoch: 000 | Batch: 2300 | Loss: 0.80034 | Correct: 394/512\n","Estimator: 000 | Epoch: 000 | Batch: 2400 | Loss: 0.83296 | Correct: 389/512\n","Estimator: 000 | Epoch: 000 | Batch: 2500 | Loss: 0.77699 | Correct: 400/512\n","Estimator: 000 | Epoch: 000 | Batch: 2600 | Loss: 0.82476 | Correct: 384/512\n","Estimator: 000 | Epoch: 000 | Batch: 2700 | Loss: 0.75999 | Correct: 399/512\n","Estimator: 000 | Epoch: 000 | Batch: 2800 | Loss: 0.79760 | Correct: 402/512\n","Estimator: 000 | Epoch: 000 | Batch: 2900 | Loss: 0.81716 | Correct: 394/512\n","Estimator: 000 | Epoch: 000 | Batch: 3000 | Loss: 0.81519 | Correct: 387/512\n","Estimator: 000 | Epoch: 000 | Batch: 3100 | Loss: 0.72459 | Correct: 406/512\n","Estimator: 000 | Epoch: 000 | Batch: 3200 | Loss: 0.76076 | Correct: 399/512\n","Estimator: 000 | Epoch: 000 | Batch: 3300 | Loss: 0.70782 | Correct: 402/512\n","Fold 5\n","2023-03-22 13:52:46.958502\n","Estimator: 000 | Epoch: 000 | Batch: 000 | Loss: 3.85462 | Correct: 16/512\n","Estimator: 000 | Epoch: 000 | Batch: 100 | Loss: 2.56088 | Correct: 150/512\n","Estimator: 000 | Epoch: 000 | Batch: 200 | Loss: 2.05553 | Correct: 196/512\n","Estimator: 000 | Epoch: 000 | Batch: 300 | Loss: 1.79845 | Correct: 235/512\n","Estimator: 000 | Epoch: 000 | Batch: 400 | Loss: 1.78023 | Correct: 257/512\n","Estimator: 000 | Epoch: 000 | Batch: 500 | Loss: 1.68808 | Correct: 262/512\n","Estimator: 000 | Epoch: 000 | Batch: 600 | Loss: 1.48923 | Correct: 278/512\n","Estimator: 000 | Epoch: 000 | Batch: 700 | Loss: 1.37924 | Correct: 292/512\n","Estimator: 000 | Epoch: 000 | Batch: 800 | Loss: 1.36071 | Correct: 301/512\n","Estimator: 000 | Epoch: 000 | Batch: 900 | Loss: 1.25238 | Correct: 325/512\n","Estimator: 000 | Epoch: 000 | Batch: 1000 | Loss: 1.18488 | Correct: 327/512\n","Estimator: 000 | Epoch: 000 | Batch: 1100 | Loss: 1.21862 | Correct: 322/512\n","Estimator: 000 | Epoch: 000 | Batch: 1200 | Loss: 1.29473 | Correct: 323/512\n","Estimator: 000 | Epoch: 000 | Batch: 1300 | Loss: 1.29733 | Correct: 321/512\n","Estimator: 000 | Epoch: 000 | Batch: 1400 | Loss: 1.12485 | Correct: 348/512\n","Estimator: 000 | Epoch: 000 | Batch: 1500 | Loss: 0.98381 | Correct: 356/512\n","Estimator: 000 | Epoch: 000 | Batch: 1600 | Loss: 1.05035 | Correct: 357/512\n","Estimator: 000 | Epoch: 000 | Batch: 1700 | Loss: 0.91522 | Correct: 374/512\n","Estimator: 000 | Epoch: 000 | Batch: 1800 | Loss: 0.96920 | Correct: 366/512\n","Estimator: 000 | Epoch: 000 | Batch: 1900 | Loss: 0.98767 | Correct: 364/512\n","Estimator: 000 | Epoch: 000 | Batch: 2000 | Loss: 0.93605 | Correct: 371/512\n","Estimator: 000 | Epoch: 000 | Batch: 2100 | Loss: 0.92552 | Correct: 372/512\n","Estimator: 000 | Epoch: 000 | Batch: 2200 | Loss: 0.89307 | Correct: 382/512\n","Estimator: 000 | Epoch: 000 | Batch: 2300 | Loss: 0.92148 | Correct: 389/512\n","Estimator: 000 | Epoch: 000 | Batch: 2400 | Loss: 0.87143 | Correct: 381/512\n","Estimator: 000 | Epoch: 000 | Batch: 2500 | Loss: 0.91309 | Correct: 388/512\n","Estimator: 000 | Epoch: 000 | Batch: 2600 | Loss: 0.84230 | Correct: 393/512\n","Estimator: 000 | Epoch: 000 | Batch: 2700 | Loss: 0.79520 | Correct: 396/512\n","Estimator: 000 | Epoch: 000 | Batch: 2800 | Loss: 0.83605 | Correct: 386/512\n","Estimator: 000 | Epoch: 000 | Batch: 2900 | Loss: 0.75963 | Correct: 392/512\n","Estimator: 000 | Epoch: 000 | Batch: 3000 | Loss: 0.88090 | Correct: 383/512\n","Estimator: 000 | Epoch: 000 | Batch: 3100 | Loss: 0.85340 | Correct: 383/512\n","Estimator: 000 | Epoch: 000 | Batch: 3200 | Loss: 0.86893 | Correct: 384/512\n","Estimator: 000 | Epoch: 000 | Batch: 3300 | Loss: 0.77731 | Correct: 392/512\n"]}],"source":["from torchensemble import VotingClassifier\n","from torchensemble.utils.logging import set_logger\n","from torchensemble.utils import io\n","from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n","from sklearn.model_selection import KFold\n","# Define the ensemble\n","currentDateAndTime = datetime.now()\n","try:\n","    os.mkdir('ensemble')\n","except:\n","    pass\n","\n","    \n","k=5\n","splits=KFold(n_splits=k,shuffle=True,random_state=42)\n","\n","for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n","    print('Fold {}'.format(fold + 1))\n","    model = VotingClassifier(\n","        estimator= Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim),\n","        n_estimators=25,\n","        cuda=True,\n","    )\n","    \n","    print(datetime.now())\n","    \n","\n","    model_path='./'\n","\n","    #io.load(model, model_path)  # reload\n","    model_path='./ensemble/'+str(fold)\n","    os.mkdir(model_path)\n","    #print(model_path)\n","    #model.load_state_dict(torch.load(model_path))\n","    # Set the criterion\n","    criterion = nn.CrossEntropyLoss()\n","    model.set_criterion(criterion)\n","\n","    # Set the optimizer\n","    model.set_optimizer('AdamW', lr=learning_rate, weight_decay=5e-4)\n","\n","    # Train and Evaluate\n","\n","\n","    \n","    train_sampler = SubsetRandomSampler(train_idx)\n","    test_sampler = SubsetRandomSampler(val_idx)\n","    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n","    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n","    # Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n","    # Initialize a model, and put it on the device specified.\n","    model.fit(\n","        train_loader,\n","        epochs=10,\n","        test_loader=val_loader,\n","        save_dir=model_path,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ab33MxosWLmG","trusted":true},"outputs":[],"source":["del train_set, val_set\n","del train_loader, val_loader\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"1Hi7jTn3PX-m"},"source":["# Testing\n","Create a testing dataset, and load model from the saved checkpoint."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from torchensemble import VotingClassifier\n","from torchensemble.utils.logging import set_logger\n","from torchensemble.utils import io\n","from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n","from sklearn.model_selection import KFold\n","models=[]\n","\n","model_pathes=['ensemble/0',\n","              'ensemble/1',\n","              'ensemble/2',\n","              'ensemble/3',\n","              'ensemble/4']\n","for i,path in enumerate(model_pathes):\n","    model = VotingClassifier(\n","            estimator= Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim),\n","            n_estimators=1,\n","            cuda=True,\n","        )\n","    model_path='./'+path\n","    io.load(model, model_path)  # reload\n","    models.append(model)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"VOG1Ou0PGrhc","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[Dataset] - # phone classes: 41, number of utterances for test: 857\n"]},{"name":"stderr","output_type":"stream","text":["857it [00:14, 59.40it/s]"]},{"name":"stdout","output_type":"stream","text":["[INFO] test set\n","torch.Size([527364, 1599])\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# load data\n","test_X = preprocess_data(split='test', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes)\n","test_set = LibriDataset(test_X, None)\n","test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ay0Fu8Ovkdad","trusted":true},"outputs":[],"source":["import pandas as pd\n","def ensemble(models,features):\n","    \n","    preds=torch.zeros(len(models)+1,features.size(0),dtype=torch.int32)\n","    #preds[0]=df\n","    for i,model in enumerate(models):\n","        model.eval()\n","        with torch.no_grad():\n","            outputs = model(features)\n","            _, val_pred = torch.max(outputs, 1) \n","            preds[i]=val_pred\n","    out=torch.zeros(features.size(0))\n","    for i in range(features.size(0)):\n","        pred=preds[:,i]\n","        #weight=pred[0]\n","        pred=torch.bincount(pred)\n","        #pred[weight]+=3\n","       \n","        sorted, indices = torch.sort(pred)\n","        '''\n","        if sorted.size(0)!=1 and sorted[0]==sorted[1]:\n","            out[i]=int(preds[0,i])\n","        else:\n","        '''\n","        out[i]=int(torch.argmax(pred))\n","        \n","    #print('out:',out.size())\n","    return out\n","    "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1031/1031 [02:50<00:00,  6.05it/s]\n"]}],"source":["\n","pred = np.array([], dtype=np.int32)\n","\n","with torch.no_grad():\n","    start=0\n","    end=0\n","    for i, batch in enumerate(tqdm(test_loader)):\n","        features = batch\n","        end+=features.size(0)\n","        features = features.to(device).reshape(-1,rnn_dim,nframes*39)\n","        #print(d.size())\n","        start=end\n","        test_pred = ensemble(models,features)\n","\n","        #_, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n","        pred = np.concatenate((pred, test_pred.cpu().numpy()), axis=0)\n","        #print(pred)\n","        \n","    \n"]},{"cell_type":"markdown","metadata":{"id":"zp-DV1p4r7Nz"},"source":["Make prediction."]},{"cell_type":"markdown","metadata":{"id":"wyZqy40Prz0v"},"source":["Write prediction to a CSV file.\n","\n","After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"GuljYSPHcZir","trusted":true},"outputs":[],"source":["with open('prediction.csv', 'w') as f:\n","    f.write('Id,Class\\n')\n","    for i, y in enumerate(pred):\n","        y=int(y)\n","        f.write('{},{}\\n'.format(i, y))"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-03-22 13:31:44.605932\n"]}],"source":["print(currentDateAndTime)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"vscode":{"interpreter":{"hash":"2f10633d0c2d43d8856f9bd12e5a0229ed2dd9aaabeb3fbc88420d061c41e2c4"}}},"nbformat":4,"nbformat_minor":4}
